{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5ecd807-fb1c-41eb-a948-365e57396d90",
   "metadata": {},
   "source": [
    "# Notebook 5: Agents & MCP Tools\n",
    "\n",
    "This notebook is for developers who are already familiar with [basic agent workflows](4.1_tool_calling.ipynb). \n",
    "Here, we will highlight more advanced use cases for agents where a single tool call is insufficient to complete the required task.\n",
    "\n",
    "We will also use [MCP tools](https://github.com/modelcontextprotocol/servers) (which can be deployed onto an OpenShift cluster) throughout this demo to show users how to extend their agents beyond Llama Stacks's current builtin tools and connect to many different services and data sources to build their own custom agents.  \n",
    "\n",
    "### Agent Example:\n",
    "\n",
    "This notebook will walkthrough how to build a system that can answer the following question via an agent built with Llama Stack:\n",
    "\n",
    "- *\"Review OpenShift logs for the failing-pod. Categorize each as either ‘Normal’ or ‘Error’. If it's an error search for a solution. Summarize any errors found.\"*\n",
    "\n",
    "### MCP Tools:\n",
    "\n",
    "#### OpenShift MCP Server\n",
    "\n",
    "Throughout this notebook we will be relying on the [Kubernetes MCP server](https://github.com/manusa/kubernetes-mcp-server) by [manusa](https://github.com/manusa) to interact with our OpenShift cluster.\n",
    "\n",
    "#### Github MCP Server\n",
    "\n",
    "Throughout this notebook we will be relying on the [Github MCP server](https://github.com/github/github-mcp-server) by Github to interact with our Github repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77a6454",
   "metadata": {},
   "source": [
    "## Setting Up this Notebook\n",
    "We will initialize our environment as described in detail in our [\"Llama Stack\" notebook](./3_Llama_Stack.ipynb). Please refer to it for additional explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafea86a-fcb4-4e69-8a73-1839b536b54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for accessing the environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# for communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client import Agent\n",
    "from llama_stack_client.lib.agents.react.agent import ReActAgent\n",
    "from llama_stack_client.lib.agents.react.tool_parser import ReActOutput\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "\n",
    "# pretty print of the results returned from the model/agent\n",
    "from termcolor import cprint\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from src.utils import step_printer\n",
    "\n",
    "base_url = os.getenv(\"REMOTE_BASE_URL\")\n",
    "\n",
    "\n",
    "client = LlamaStackClient(base_url=base_url)\n",
    "\n",
    "print(f\"Connected to Llama Stack server\")\n",
    "\n",
    "# model_id for the model you wish to use that is configured with the Llama Stack server\n",
    "model_id = \"llama3-2-3b\" # \"deepseek-r1-0528-qwen3-8b-bnb-4bit\"\n",
    "\n",
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "if temperature > 0.0:\n",
    "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "max_tokens = 5000\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "stream = False\n",
    "\n",
    "print(f\"Inference Parameters:\\n\\tModel: {model_id}\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66044170",
   "metadata": {},
   "source": [
    "## Validate tools are available in our Llama Stack instance\n",
    "\n",
    "When an instance of Llama Stack is redeployed, it may be the case that the tools will need to be re-registered. Also if a tool is already registered with a Llama Stack instance, trying to register another one with the same `toolgroup_id` will throw you an error.\n",
    "\n",
    "For this reason, it is recommended to validate your tools and toolgroups. The following code will check that both the `builtin::websearch` and `mcp::openshift` tools are correctly registered, and if not it will attempt to register them using their specific endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2cedaf-522b-4251-886a-d8aa7b9fcd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocp_mcp_url = os.getenv(\"REMOTE_OCP_MCP_URL\")\n",
    "\n",
    "registered_tools = client.tools.list()\n",
    "registered_toolgroups = [t.toolgroup_id for t in registered_tools]\n",
    "if \"mcp::openshift\" not in registered_toolgroups:\n",
    "    client.toolgroups.register(\n",
    "        toolgroup_id=\"mcp::openshift\",\n",
    "        provider_id=\"model-context-protocol\",\n",
    "        mcp_endpoint={\"uri\":ocp_mcp_url},\n",
    "    )\n",
    "\n",
    "print(f\"Your Llama Stack server is registered with the following tool groups @ {set(registered_toolgroups)} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea60a1b9",
   "metadata": {},
   "source": [
    "## Defining our Agent - ReAct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd7dbe5",
   "metadata": {},
   "source": [
    "Now that we've shown that we can successfully accomplish this multi-step multi-tool task using prompt chaining, let's see if we can give our agent a bit more autonomy to perform the same task but with a single prompt instead of a chain. To do this, we will instantiate a **ReAct agent** (which is included in the llama stack python client by default).The ReAct agent is a variant of the simple agent but with the ability to loop through \"Reason then Act\" iterations, thinking through the problem and then using tools until it determines that it's task has been completed successfully.  \n",
    "\n",
    "Unlike prompt chaining which follows fixed steps, ReAct dynamically breaks down tasks and adapts its approach based on the results of each step. This makes it more flexible and capable of handling complex, real-world queries effectively.\n",
    "\n",
    "Below you will see the slight differences in the agent definition and the prompt used to accomplish our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50e9f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"deepseek-r1-0528-qwen3-8b-bnb-4bit\"\n",
    "stream = True\n",
    "\n",
    "#model_id = \"llama3-2-3b\"\n",
    "#stream = False\n",
    "\n",
    "agent = ReActAgent(\n",
    "            client=client,\n",
    "            model=model_id,\n",
    "            tools=[\"mcp::openshift\", \"builtin::websearch\"],\n",
    "            response_format={\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": ReActOutput.model_json_schema(),\n",
    "            },\n",
    "            sampling_params={\"max_tokens\":512},\n",
    "        )\n",
    "user_prompts =[\"\"\"Review the OpenShift logs for the pod 'slack-test' with a container of the same name,in the 'test' namespace.\"\n",
    "                If the logs indicate an error search for the top OpenShift solution. Create a summary message with the category and explanation of the error.\"\"\"]\n",
    "session_id = agent.create_session(\"web-session\")\n",
    "for prompt in user_prompts:\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    cprint(f\"Processing user query: {prompt}\", \"blue\")\n",
    "    print(\"=\"*50)\n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "        stream=stream\n",
    "    )\n",
    "    if stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        step_printer(response.steps) # print the steps of an agent's response in a formatted way. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3998aab7",
   "metadata": {},
   "source": [
    "### Output Analysis\n",
    "\n",
    "Above, we can see that the ReAct agent took nearly an identical approach to the prompt chaining method above, but using a single prompt instead of a chain.  \n",
    "\n",
    "1. First the LLM generated a tool call for the `pods_log` tool included in the **OpenShift MCP server** and fetched the logs for the specified pod.\n",
    "2. The tool successfully retrieved the logs for the pod.\n",
    "3. The LLM  then received the logs from the tool call, along with the original query.\n",
    "4. This context was then passed back to the LLM for the final inference. The inference result provided a summary of the pod logs along with its category of 'Normal' or 'Error'.\n",
    "5. Next the LLM generates a tool call for the default builtin `brave_search` tool which was not available. This is because the models have been trained with Brave Search as a built-in tool.\n",
    "6. Next the LLM generates a tool call for the `web_search` tool instead looking for the top answer to the error.\n",
    "7. A summary of the pod log error and possible next steps to help solve the problem are suggested. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecbc8ab-77c6-48ff-970c-2d5dfd54a2c7",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "This notebook demonstrated how to build an agentic MCP applications with Llama Stack. We did this by initializing an agent with access to two MCP servers that were registered to our Llama Stack server, then invoked the agent on our specified set of queries. We showed that we can do this with more directed Prompt Chaining or with the more open ended ReAct pattern."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
