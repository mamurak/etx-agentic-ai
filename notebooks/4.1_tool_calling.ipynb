{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4.1: Simple Agent with Tool Calling\n",
    "\n",
    "This notebook will introduce how to build a simple agent using Llama Stack's agent framework, enhanced with a single tool: the builtin web search tool. This capability will  allow the agent to retrieve up to date external information beyond the limits of its training data. This is an important step toward developing a more capable and autonomous agent. Make sure you have setup your environment OK in [Level 1](Level1_getting_started_with_Llama_Stack.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial will walk you through how to build your own AI agent who can search the web:\n",
    "\n",
    "1. Configure a Llama Stack agent.\n",
    "2. Enhance the agent by providing it access to a specific tool\n",
    "2. Interact with the agent and tests its use of the web search tool.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting this notebook, ensure that you have:\n",
    "- Followed the instructions in the [Setup Guide](./3_Llama_Stack.ipynb) notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up this Notebook\n",
    "We will start with a few imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import Agent\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "from src.client_tools import get_pod_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Testing the tool function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will initialize our environment as described in detail in our [\"Getting Started\" notebook](./Level0_getting_started_with_Llama_Stack.ipynb). Please refer to it for additional explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from kubernetes.client.api_client import ApiClient\n",
    "from kubernetes.client.rest import ApiException\n",
    "from kubernetes.client import CoreV1Api\n",
    "from kubernetes.config import load_incluster_config\n",
    "from llama_stack_client.lib.agents.client_tool import client_tool\n",
    "\n",
    "\n",
    "load_incluster_config()\n",
    "api_instance = CoreV1Api(ApiClient())\n",
    "\n",
    "\n",
    "def get_pod_log_test(pod_name: str, namespace: str, container_name: str):\n",
    "    \"\"\"\n",
    "    Provide the location upon request.\n",
    "\n",
    "    :param pod_name: The name of the target pod\n",
    "    :param namespace: The name of the target namespace\n",
    "    :param container_name: The name of the target container within the target pod\n",
    "    :returns: Logs of the target pod\n",
    "    \"\"\"\n",
    "    try:\n",
    "        api_response = api_instance.read_namespaced_pod_log(\n",
    "            pod_name, namespace, container=container_name, tail_lines=100\n",
    "        )\n",
    "        pprint(api_response)\n",
    "    except ApiException as e:\n",
    "        print(\"Exception when calling CoreV1Api->read_namespaced_pod_log: %s\\n\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pod_name = 'java-app-build-iyyx5z-build-pod'\n",
    "namespace = 'demo-application'\n",
    "container_name = 'step-s2i-build'\n",
    "\n",
    "get_pod_log_test(pod_name, namespace, container_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure an agent for tool use.\n",
    "\n",
    "- **Agent Initialization**: First we create an `Agent` instance with the desired LLM model, agent instructions and tools.\n",
    "\n",
    "- **Instructions**: The `instructions` parameter, also referred to as the system prompt, specifies the agent's role and behavior. In this example, the agent is configured as a helpful web search assistant. It is instructed to use a tool whenever a web search is required and to respond in a friendly and helpful tone.\n",
    "\n",
    "- **Tools**: The `tools` parameter defines the tools available to the agent. In this case, the `get_pod_log` tool is used, which enables the agent to look up logs from a pod.\n",
    "\n",
    "- **How It Works**: When a user query is provided, the agent processes the input and determines whether a tool is required to fulfill the request. If the query involves retrieving logs from a pod, the agent invokes the `get_pod_log` tool. The tool interacts with the Kubernetes API server to fetch the container logs. This workflow ensures that the agent can handle a wide range of queries effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for accessing the environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# for communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# pretty print of the results returned from the model/agent\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from src.utils import step_printer\n",
    "from termcolor import cprint\n",
    "\n",
    "base_url = os.getenv(\"REMOTE_BASE_URL\")\n",
    "\n",
    "client = LlamaStackClient(base_url=base_url)\n",
    "\n",
    "print(f\"Connected to Llama Stack server\")\n",
    "\n",
    "# model_id for the model you wish to use that is configured with the Llama Stack server\n",
    "model_id = 'granite-31-2b-instruct' # \"deepseek-r1-0528-qwen3-8b-bnb-4bit\"\n",
    "\n",
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "if temperature > 0.0:\n",
    "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "max_tokens = 6000\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "stream = \"True\"\n",
    "\n",
    "print(f\"Inference Parameters:\\n\\tModel: {model_id}\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    client, \n",
    "    model=model_id,\n",
    "    instructions=\"\"\"You are a helpful assistant. \n",
    "    When a user asks for pod logs, you MUST use the get_pod_log tool.\n",
    "    \"\"\" ,\n",
    "    tools=[get_pod_log],\n",
    "    sampling_params=sampling_params\n",
    ")\n",
    "pod_name = 'java-app-build-iyyx5z-build-pod'\n",
    "namespace = 'demo-application'\n",
    "container_name = 'step-s2i-build'\n",
    "user_prompts = [\n",
    "    f\"What did the {container_name} container within the {pod_name} pod in namespace {namespace} log?\"\n",
    "]\n",
    "session_id = agent.create_session(\"tool-test-session\")\n",
    "for prompt in user_prompts:\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    cprint(f\"Processing user query: {prompt}\", \"blue\")\n",
    "    print(\"=\"*50)\n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "        stream=stream\n",
    "    )\n",
    "\n",
    "    if stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        step_printer(response.steps) # print the steps of an agent's response in a formatted way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- We've demonstrated how to set up Llama Stack agents and extended them with builtin tools (like web search) that come prepackaged with Llama Stack.\n",
    "- We've shown that this simple approach can provide significantly increased functionality of existing open source LLM's. \n",
    "- This will serves as a foundational example for the more advanced examples to come involving Agentic RAG, External Tools, and complex agentic patterns.\n",
    "\n",
    "Continue to the [next notebook](./4.2_prompt_chaining.ipynb) to learn how we can upgrade our agents to solve even more complex and multi-step tasks using advanced agentic patterns. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
