{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce5ac225-ad97-4cbd-8e90-f0ad64cbf00b",
   "metadata": {},
   "source": [
    "# Notebook 3: Getting Started with Llama Stack\n",
    "\n",
    "This notebook will help you set up your environment for this tutorial. Specifically, we will cover installing the necessary libraries, configuring essential parameters, and connecting to a Llama Stack server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf06a5b-7259-4e4b-94ef-fbb577c30f9e",
   "metadata": {},
   "source": [
    "## Installing Dependencies\n",
    "\n",
    "This code requires `llama-stack` and the `llama-stack-client` python packages. Let's begin by installing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0ddc8b-d6cf-4cb3-8678-7b52ee70131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qr requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6589b1ae-9fd5-4715-a630-cc3cc52035e9",
   "metadata": {},
   "source": [
    "## Setting the Environment Variables\n",
    "\n",
    "We will symlink the [`env.example`](../../../env.example) file to create a new file called `.env`. We've included as many reasonable defaults as possible to get you started, but please use this file to make any customizations needed for your environment such as the the location of the Llama Stack server endpoint..\n",
    "\n",
    "### Environment variables required for all demos\n",
    "- `REMOTE_BASE_URL`: the URL of the remote Llama Stack server.\n",
    "- `TEMPERATURE` (optional): the temperature to use during inference. Defaults to 0.0.\n",
    "- `TOP_P` (optional): the top_p parameter to use during inference. Defaults to 0.95.\n",
    "- `MAX_TOKENS` (optional): the maximum number of tokens that can be generated in the completion. Defaults to 512.\n",
    "- `STREAM` (optional): set this to True to stream the output of the model/agent and False otherwise. Defaults to False.\n",
    "- `REMOTE_OCP_MCP_URL`: the URL for your Openshift MCP server. If the client does not find the tool registered to the llama-stack instance, it will use this URL to register the Openshift tool.\n",
    "- `REMOTE_GITHUB_MCP_URL`: the URL for your Github MCP server. If the client does not find the tool registered to the llama-stack instance, it will use this URL to register the Github tool.\n",
    "- `USE_PROMPT_CHAINING`: dictates if the prompt should be formatted as a few separate prompts to isolate each step or in a single turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d287946d-a921-49ae-af8b-880a056538f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -s env.example .env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573899bf-34cb-4f31-9fc1-48aca439fc60",
   "metadata": {},
   "source": [
    "## Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b09511e-f6dc-4a54-b037-d64adbeaa8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for accessing the environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "from rich import print\n",
    "\n",
    "# for communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client.types import UserMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeb65ed-f368-4bb5-957b-6698fe85b829",
   "metadata": {},
   "source": [
    "## Setting Up the Server Connection\n",
    "\n",
    "Establish the connection to your Llama Stack server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60fb3f3-4d04-4916-84fc-f798b059ff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = os.getenv(\"REMOTE_BASE_URL\", \"http://llamastack-server:8321\")\n",
    "model_id = 'granite-31-2b-instruct'\n",
    "\n",
    "client = LlamaStackClient(base_url=base_url)\n",
    "\n",
    "print(base_url)\n",
    "print(client.models.list())\n",
    "print(f\"Connected to Llama Stack server\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80f5ae8-8083-4896-a973-f310df129ec6",
   "metadata": {},
   "source": [
    "## Initializing the Inference Parameters\n",
    "\n",
    "Fetch the inference-related parameters from the corresponding environment variables and convert them to the format Llama Stack expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0092359-a5db-4d9a-a735-bb931ba05f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "if temperature > 0.0:\n",
    "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "max_tokens = int(os.getenv(\"MAX_TOKENS\", 4096))\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "stream_env = os.getenv(\"STREAM\", \"True\")\n",
    "# the Boolean 'stream' parameter will later be passed to Llama Stack Agents/Inference APIs\n",
    "# any value non equal to 'False' will be considered as 'True'\n",
    "stream = (stream_env != \"False\")\n",
    "print(f\"Inference Parameters:\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cdd34a",
   "metadata": {},
   "source": [
    "Now, let's use the Llama stack inference API to greet our LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4423b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = UserMessage(\n",
    "    content=\"Hi, how are you?\",\n",
    "    role=\"user\",\n",
    ")\n",
    "client.inference.chat_completion(\n",
    "    model_id=model_id,\n",
    "    messages=[message],\n",
    "    sampling_params=sampling_params,\n",
    "    stream=stream\n",
    ").completion_message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d0a375",
   "metadata": {},
   "source": [
    "# Next\n",
    "\n",
    "Now that we've connected to Llama Stack, let's get started building the agentic AI system! The next notebook will teach you how to build a [simple agent with tool calling](./4.1_tool_calling.ipynb) application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3858040-7de0-4275-aebe-aefd3d95a0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
