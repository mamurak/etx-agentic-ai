= Using Llama Stack

== LLamaStack

[LLamaStack](https://llama-stack.readthedocs.io/en/latest/) is the open-source framework for building generative AI applications. We have a LLamaStack server configured for our use and we are going to take a look around using the playground UI.

If you are intersted in how to configure LLamaStack, checkout the [docs](https://llama-stack.readthedocs.io/en/latest/) and you can also take a look at the running config in your cluster:

<a href="https://console-openshift-console.apps.sno.<CLUSTER_DOMAIN>/k8s/ns/agent-demo/configmaps/run-config" target="_blank">ConfigMap run-config</a>

== Models

We have three models configured in LLamaStack. Two of them are available to `Chat` to i.e. are type `llm` in the model dropdown. The third model is an embedding model.

```yaml
models:
- metadata: {}
  model_id: ${env.LLAMA3B_MODEL}
  provider_id: llama-3b
  model_type: llm
- metadata: {}
  model_id: ${env.DEEPSEEK_MODEL}
  provider_id: deepseek
  model_type: llm
- metadata:
    embedding_dimension: 384
  model_id: all-MiniLM-L6-v2
  provider_id: sentence-transformers
  model_type: embedding
```

![images/model-intro.png](images/model-intro.png)

The models are small in terms of parameter size (3b and 8b) and the DeepSeek model is quantized to 4-bit. This reduces the amount of memory they consume on the GPU. Even so we are using 18Gi of nvram to run the LLM's. If you browse to the <a href="https://console-openshift-console.apps.sno.<CLUSTER_DOMAIN>/k8s/ns/llama-serving/core~v1~Pod" target="_blank">Deepseek Pod</a> enter the `Terminal` and run `nvtop` on the command line to see the GPU performance.

![images/model-gpu.png](images/model-gpu.png)

== Chat

You can chat with either of the two LLM models. You should note that the LLama model is an [instruction tuned model](https://huggingface.co/meta-llama/Llama-3.2-3B) whilst the Deepseek model is a [reasoning model](https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-bnb-4bit). Deepseek will produce reasoning tokens (between \<think\>) where it will try to break a problem down into steps prior to generating output tokens.

![images/chat-intro.png](images/chat-intro.png)

= Using Llama Stack operator

= Llama Stack K8s Operator

Github upstream link: https://github.com/llamastack/llama-stack-k8s-operator

The Llama Stack K8s Operator is a Kubernetes operator that automates the deployment and management of Llama Stack servers in Kubernetes environments. It provides a declarative way to manage AI model serving infrastructure.
It is:
1. Kubernetes native and follows standard operator development pattern.
2. Supports Ollama and vLLM inference providers.
3. Allows configuring and managing Llama stack servers and client instances.

== LlamaStackDistribution Custom Resource

The `LlamaStackDistribution` is the main custom resource that defines how a Llama Stack server should be deployed. It allows you to specify:

* **Server Configuration**: Which distribution to use (Ollama, vLLM, etc.)
* **Container Specifications**: Port, environment variables, resource limits

=== Example LlamaStackDistribution

```yaml
apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
   name: llamastack-with-config
spec:
   replicas: 1
   server:
     distribution:
       name: remote-vllm
     containerSpec:
       port: 8321
     userConfig:
        # reference to the configmap that contains Llama stack configuration.
       configMapName: llama-stack-config 
```

== Using ConfigMap for run.yaml Configuration

The operator supports using ConfigMaps to store `run.yaml` (https://github.com/meta-llama/llama-stack/blob/main/llama_stack/templates/vllm-gpu/run.yaml) configuration file. 

* **Centralized Configuration**: Store all Llama Stack settings in one place
* **Dynamic Updates**: Changes to the ConfigMap automatically restart pods to load new configuration
* **Environment-Specific Configs**: Use different ConfigMaps for different environments

=== ConfigMap Example

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: llamastack-config
data:
  run.yaml: |
    # Llama Stack configuration
    providers:
      - name: vllm
        url: <url>
        models:
          - name: llama3.2:1b
            context_length: 4096
    tools:
      - name: builtin::websearch
        enabled: true
      - name: mcp::openshift
        enabled: true
```

== Integration with Workshop

In this CI/CD agent workshop, you will use the Llama Stack K8s Operator to:

1. **Deploy Llama Stack Servers**: Use the operator to quickly provision Llama Stack instances with your preferred models
2. **Configure Tools**: Set up builtin::websearch and MCP tools through ConfigMaps
3. **Manage Multiple Environments**: Use different ConfigMaps for development, testing, and production
4. **Automate Deployment**: Integrate the operator deployment into your CI/CD pipelines
