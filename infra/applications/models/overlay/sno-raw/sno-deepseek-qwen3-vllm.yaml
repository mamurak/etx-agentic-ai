object-templates-raw: |
  {{- if not (lookup "v1" "ConfigMap" "llama-serving" "undeploy-sno-deepseek-qwen3-vllm") }}
  - complianceType: musthave
    objectDefinition:
      apiVersion: serving.kserve.io/v1beta1
      kind: InferenceService
      metadata:
        annotations:
          argocd.argoproj.io/sync-options: SkipDryRunOnMissingResource=true
          argocd.argoproj.io/sync-wave: "4"
          openshift.io/display-name: sno-deepseek-qwen3-vllm
          serving.knative.openshift.io/enablePassthrough: "true"
          serving.kserve.io/deploymentMode: RawDeployment
          sidecar.istio.io/inject: "false"
          sidecar.istio.io/rewriteAppHTTPProbers: "true"
        labels:
          opendatahub.io/dashboard: "true"
        name: sno-deepseek-qwen3-vllm
        namespace: llama-serving
      spec:
        predictor:
          maxReplicas: 1
          minReplicas: 1
          model:
            modelFormat:
              name: vLLM
            name: ""
            resources:
              limits:
                nvidia.com/gpu: "1"
              requests:
                nvidia.com/gpu: "1"
            runtime: sno-deepseek-qwen3-vllm
            storageUri: oci://quay.io/eformat/deepseek-r1-0528-qwen3-8b-bnb-4bit:latest-ubi
          tolerations:
          - effect: NoSchedule
            key: nvidia.com/gpu
            operator: Exists
  {{- end }}
