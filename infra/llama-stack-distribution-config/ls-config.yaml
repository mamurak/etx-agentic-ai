apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-stack-config
data:
  run.yaml: |
    # Llama Stack Configuration
    version: '2'
    image_name: vllm
    apis:
    - inference
    providers:
      inference:
      - provider_id: vllm
        provider_type: "remote::vllm"
        config:
          url: "${VLLM_URL}"
          model: "granite-31-2b-instruct"
    tool_groups:
    - provider_id: tavily-search
      toolgroup_id: builtin::websearch
    - toolgroup_id: builtin::rag
      provider_id: rag-runtime
    - toolgroup_id: mcp::openshift
      provider_id: model-context-protocol
      mcp_endpoint:
        uri: ${MCP_URL_OPENSHIFT}
    - toolgroup_id: mcp::github
      provider_id: model-context-protocol
      mcp_endpoint:
        uri: ${MCP_URL_GITHUB}
    server:
      port: 8321
---
apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: llamastack-with-config
spec:
  replicas: 1
  server:
    distribution:
      name: remote-vllm
    containerSpec:
      port: 8321
      env:
      - name: INFERENCE_MODEL
        value: "granite-31-2b-instruct"
      - name: VLLM_URL
        value: ${VLLM_URL}"
    userConfig:
      configMapName: llama-stack-config
      # configMapNamespace: ""  # Optional - defaults to the same namespace as the CR

